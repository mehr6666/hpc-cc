<?xml version="1.0" encoding="UTF-8"?>
<!--
<!DOCTYPE article SYSTEM "docbookx.dtd">
-->
<article lang="ru">
    <articleinfo>
	<title>Правила пользования ресурсом hpc.cc.spbu.ru</title>
	<authors>
	    <author>
		<firstname>Владимир</firstname>
		<surname>Гайдучок</surname>
		<othername role="middle">Юрьевич</othername>
		<email>gvladimiru@gmail.com</email>
	    </author>
	    <author>
		<firstname>Иван</firstname>
		<surname>Ганкевич</surname>
		<othername role="middle">Геннадьевич</othername>
		<email>gig.spb@gmail.com</email>
	    </author>
	    <author>
		<firstname>Андрей</firstname>
		<surname>Зароченцев</surname>
		<othername role="middle">Константинович</othername>
		<email>andrey.zar@gmail.com</email>
	    </author>
	    <author>
		<firstname>Виталий</firstname>
		<surname>Лысов</surname>
		<othername role="middle">Константинович</othername>
		<email>vij@ptc.spbu.ru</email>
		<affiliation>
		 <jobtitle>программист</jobtitle>
		 <orgname>УСИТ СПбГУ</orgname>
		</affiliation>
	    </author>
	</authors>
	<copyright>
	    <year>2011</year>
	    <holder>Гайдучок Владимир, Ганкевич Иван, Зароченцев Андрей, Лысов Виталий</holder>
	</copyright>
	<collabname>Лысов Виталий</collabname>
	<revhistory>
	 <revision>
	  <revnumber>0.1</revnumber>
	  <date>2011-01-23</date>
	  <authorinitials>ЗАК</authorinitials>
	  <revremark>Черновая версия</revremark>
	 </revision>
	 <revision>
	  <revnumber>1.1</revnumber>
	  <date>2011-05-11</date>
	  <authorinitials>ЛВК</authorinitials>
	  <revremark>Черновая docbook-версия</revremark>
	 </revision>
	 <revision>
          <revnumber>2.0</revnumber>
          <date>2011-06-27</date>
          <authorinitials>ЛВК</authorinitials>
          <revremark>Опубликовано на сайте</revremark>
         </revision>
	</revhistory>

    </articleinfo>
    <abstract>
    <para>
Сервис создан в дополнение к сервису <quote>Виртуальная машина</quote> (ВМ) для проведения расчетов по программам,
требующих большую вычислительную мощность (терафлопного уровня). Основные работы по отладке программ, 
подготовки данных и визуализации результатов проводятся на ВМ (рабочая среда). Выполнение расчетов на 
высокопроизводительном кластере проводится с использованием системы очередей. 
    </para>
    </abstract>
<!--
<section id="intro">
<title>Описание сервиса "Высокопроизводительные вычисления"</title>
-->
    <section id="vm">
    <title>Виртуальная машина</title>
    <para>
Доступ к вычислительным ресурсам осуществляется через виртуальную машину пользователя. Назначение ВМ следующее:
    </para>
<itemizedlist>
    <listitem>
	<para>Хранение обрабатываемых данных</para>
    </listitem>
    <listitem>
	<para>Разработка приложений</para>
    </listitem>
    <listitem>
	<para>Проведение вычислений</para>
    </listitem>
</itemizedlist>
<para>
Для получения виртуальной машины необходимо заполнить <ulink url="http://www.ptc.spbu.ru/zayavlenie_vm.doc" >заявку</ulink>, 
которую следует отправить по адресу noc@ptc.spbu.ru.
По умолчанию заводится виртуальная машина со следующими характеристиками:
</para>
<itemizedlist>
    <listitem>
	<para>ОС: CentOS 5.6</para>
    </listitem>
    <listitem>
	<para>Количество ядер CPU: 4</para>
    </listitem>
    <listitem>
	<para>ОЗУ: 8Гб</para>
    </listitem>
    <listitem>
	<para>Жёсткий диск: /home &#160;&#8212; 50 Гб, системные разделы &#160;&#8212; 8 Гб. Управление разбиением &#160;&#8212; lvm.</para>
    </listitem>
</itemizedlist>
Список официально поддерживаемых операционных систем (ОС) опубликован в 
<ulink url="http://www.vmware.com/pdf/vsphere4/r40/vsp_compatibility_matrix.pdf" >документе</ulink> (Таблица 14, столбец Guest OS Cust).
Не требуют согласования заявки на виртуальные машины со следующими предустановленными ОС: 
SLES 11 SP1, CentOS 5.5, Scientific Linux 5.5, Debian 5, Debian 6, Ubuntu 10.04 LTS, Fedora 14.

Установку коммерческих программных продуктов, требующих наличия лицензий, например, 
RHEL 5, RHEL 6 и т.д. необходимо предварительно согласовать со службой технической поддержки &#160;&#8212; noc@ptc.spbu.ru.

В системе заводится пользователь username, которого основан на фамилии и имени человека подавшего заявку на получение 
виртуальной машины. Наделён правами постановки задач в очереди на имеющиеся HPC-ресурсы. При старте задачи его 
домашний каталог монтируется на соответствующие вычислительные узлы.
<!-- В системе заводятся дополнительно два пользователя: 
<itemizedlist>
    <listitem>
	<para>vmuser &#160;&#8212; пользователь с административными (средствами <command>sudo</command>) правами, </para>
    </listitem>
    <listitem>
	<para>Пользователь username, которого основан на фамилии и имени человека подавшего заявку на получение 
  виртуальной машины. Наделён правами постановки задач в очереди на имеющиеся HPC-ресурсы. При старте задачи его 
домашний каталог монтируется на соответствующие вычислительные узлы.</para>
    </listitem>
</itemizedlist>
-->
</section>

    <section id="queues">
    <title>Типы очередей</title>
<para>
Имеются 3 типа очередей для ТП-кластера &#160;&#8212; короткая short, длинная long, и бесконечная infi. <!-- 1 очередь для доступа на виртуальный кластер &#160;&#8212;  virt. -->Каждая из них имеет свои особенности.
</para>
    <section id="short">
    <title>short</title>
<itemizedlist>
  <listitem><para>Высокий приоритет (задачи меньше стоят в очереди)</para></listitem>
  <listitem><para>Процессорное время ограничено 6-ю часами</para></listitem>
  <listitem><para>Память ограничена 2 ГБ на процесс</para></listitem>
  <listitem><para>Реальное время ограничено 9 часами</para></listitem>
  <listitem><para>Количество задач разрешенных на выполнение одним пользователем &#160;&#8212; 3</para></listitem>
  <listitem><para>Количество задач в очереди от одного пользователя 6</para></listitem>
</itemizedlist>
</section>

    <section id="long">
    <title>long</title>

<itemizedlist>
 <listitem><para>Низкий приоритет (задачи дольше стоят в очереди)</para></listitem>
 <listitem><para>Процессорное время ограничено 48-ю часами</para></listitem>
 <listitem><para>Память ограничена 2 ГБ на процесс</para></listitem>
 <listitem><para>Реальное время ограничено 72-мя часами</para></listitem>
 <listitem><para>Количество задач разрешенных на выполнение одним пользователем &#160;&#8212; 3</para></listitem>
 <listitem><para>Количество задач в очереди от одного пользователя 6</para></listitem>
</itemizedlist>
</section>

    <section id="infi">
    <title>infi</title>

<itemizedlist>
 <listitem><para>Самый низкий приоритет (задачи дольше всех  стоят в очереди)</para></listitem>
 <listitem><para>Процессорное время не ограничено</para></listitem>
 <listitem><para>Память ограничена 2 ГБ на процесс</para></listitem>
 <listitem><para>Реальное время не ограничено</para></listitem>
 <listitem><para>Количество задач разрешенных на выполнение одним пользователем &#160;&#8212; 3</para></listitem>
 <listitem><para>Количество задач в очереди от одного пользователя 6</para></listitem>
</itemizedlist>
</section>
<!--
    <section id="virt">
    <title>virt</title>

<itemizedlist>
 <listitem><para>Процессорное время ограничено 48-ю часами</para></listitem>
 <listitem><para>Память ограничена 3 ГБ на процесс</para></listitem>
 <listitem><para>Реальное время ограничено 72-мя часами</para></listitem>
 <listitem><para>Количество задач разрешенных на выполнение одним пользователем &#160;&#8212; 3</para></listitem>
 <listitem><para>Количество задач в очереди от одного пользователя 6</para></listitem>
</itemizedlist>
</section>
-->
<para>
Доступ к очередям определяется конкретным договором (пользователем, задачей).
Для вычислений можно использовать 64 ядра (это 16 процессоров или 8 узлов).
Всего имеется 368 ядер, 46 узлов, 92 процессора.
Для обеспечения связи между узлами используются 1 Gb Eth и 10 Gb IB.
Оперативная память на узлах 16 GB (2GB на ядро) + 16GB swap.
Доступная дисковая память на ноде (<filename>/tmp</filename>) 3GB на процесс.
Дисковая память в домашней директории (<filename>/home/$username</filename>) определяется конфигурацией ВМ и не рекомендуется для 
записи временных файлов из-за метода монтирования этой директории на рабочие узлы.

Связь нода - pbs-сервер происходит через форвардинг между инетрфейсам ib-eth, что
позволяет не задействовать дополнительную машину с IB интерфейсом в качестве
сервера или роутера.
</para>
</section>

    <section id="run-tasks">
    <title>Запуск задач</title>
<note>
Все рассматриваемые скрипты и исходные коды программ доступны вам и располагаются в домашней директории пользователя (<command>~/</command>), имеющего доступ к постановке задач в очередь для счёта на кластере. Отсутствующий файл вы можете скачать <ulink url="http://v105.ptc.spbu.ru/arch/user/">здесь</ulink>.
</note>
    <section id="pbs-scripts">
    <title>Скрипты для управления задачами</title>
<para>
Для пользования сервисом используются скрипты, в которых прописано необходимые параметры.
Для самостоятельного использования интерфейса torque читайте часть 3, с необходимыми параметрами. 
</para>
Для запуска задач используется скрипт <filename>submit-tp</filename>.
<screen><![CDATA[
$ ~/submit-tp
usage:  
        --input -i filename             #input file
        --np -n N                       #number of threads. Default 1
        --dir -d dirname                #exec dirname
        --file -f filename              #exec file
        --queue -q queue                #queue name (short, long or infi). Default short
        --mpiver -v version             #MPI version (openmpi,mpich2,openmpi32). Defaul without mpi
        --output -o  filename           #output file
        --help -h                       #This message
        --jobname -j jobname            #Name of job. Default job.1308734148
        --array -r numbers              #numbers jobs in array
        --debug                         #Debug option
	--count -c                      #count by user (for advanced)
        --nodelimit limit               #limit jobs per node (fore more RAM per cpu)
        --mail -m a(be)                 #Sends email to the submitter when the job aborts (a), begins(b),  ends (e)
]]></screen>

Для мониторинга задач используется скрипт <filename>status-tp</filename>.
<screen><![CDATA[
$ ~/status-tp --help
        -f jobaname     #inforamtion abou "jobname"
        -q queue        #information about "queue"
        -a              #information about all queues
        -p              #information about cluster
        -h              #this message
]]></screen>
</section>

    <section id="start-tasks">
    <title>Примеры запуска задач</title>

<section id="ex_hello-world">
<title>Запуск скрипта <quote>Hello World</quote> на 1 ядре</title>
<screen><![CDATA[
$ ~/submit-tp -f ~/examples/helloworld.sh -m abe
2880.pbs-tp.hpc.cc.spbu.ru

$ ll *2880
-rw------- 1 zar users 11 Jan 26 11:19 job.1296029939.e2880
-rw------- 1 zar users 11 Jan 26 11:19 job.1296029939.o2880
]]></screen>
Ключ -m abe означает, что Вам на почту, указанную при регистрации, приходит письмо с репортом о состоянии Вашей задачи: a - abort, b - begin, e - end.
</section>

<section id="ex_intel">
<title>Запуск компиляторов <orgname>Intel</orgname></title>
<screen><![CDATA[
~/submit-tp -f ~/examples/intel.sh
]]></screen>
В результате вы получите информацию о версиях соответствующих компиляторов.
Содержимое скрипта <filename>~/examples/intel.sh</filename>:
<programlisting>
#/bin/sh!
. /usr/local/intel/mkl/bin/mklvars.sh intel64
. /usr/local/intel/bin/compilervars.sh intel64
icc -V
icpc -V
ifort -V
</programlisting>
</section>

<section id="ex_compilers_intel_ics">
	<title>Запуск программ из пакета <orgname>Intel Cluster Studio</orgname></title>
Перед использованием инструментов ICS необходимо инициализировать переменные окружения:
<screen><![CDATA[
. ~/examples/intel-cluster-studio.sh
]]></screen>
После этого можно компилировать программы следующим образом:
<screen><![CDATA[
$ cd ~/examples
$ cat main.c 
#include <stdio.h>
int main()
{
	printf("Hello\n");
	return 0;
}
$ icc main.c -o main
$ ./main
Hello
]]></screen>
Для профилирования MPI приложений на кластере можно воспользоваться скриптом <filename>trace-mpi.sh</filename>, например:
<screen><![CDATA[
$ . ~/examples/intel-cluster-studio.sh
$ cd ~/examples
$ mpiicc hello_mpi.c -o hello_mpi
$ ~/submit-tp -n 2 -d `pwd` -f "~/examples/trace-mpi.sh `pwd`/hello_mpi"
19490.pbs-tp.hpc.cc.spbu.ru
$ tail -2 *o19490
Trace directory: /home/igankevich/examples/trace-mpi-2012.01.10-16.03.26
TP
$ traceanalyzer trace-mpi-2012.01.10-16.03.26/*stf
]]></screen>
</section>




<section id="ex_pgi">
<title>Запуск компиляторов <orgname>PGI</orgname></title>

<!--
<para>Установка. Для установки компиляторов на конкретную машину необходимо выполнить следующие команды.
<screen><![CDATA[
$ . /usr/local/pgi/linux86-64/11.10/pgi.sh
$ sudo mkdir -p /usr/pgi/11.10/share_objects
$ sudo makelocalrc -x /usr/local/pgi/linux86-64/11.10 -net /usr/pgi/11.10/share_objects
/usr/local/pgi/linux86-64/11.10/bin is not writable
$ ls /usr/pgi/11.10/share_objects/lib64/libnuma.so
/usr/pgi/11.10/share_objects/lib64/libnuma.so
$ cp /usr/local/pgi/linux86-64/11.10/bin/localrc.wind ~/localrc.`hostname`
]]></screen>
</para>
-->
<para>Запуск на виртуальной машине. Перед запуском компилятора PGI необходимо установить переменные окружения, затем воспользоваться одной из команд для соответствующего языка программирования (pgCC для С++, pgcc для C, pgfortran для Fortran). Например:
<screen><![CDATA[
$ . /usr/local/pgi/linux86-64/11.10/pgi.sh
$ pgf77 ~/examples/Pi/Pi.f -o ~/examples/Pi/Pi-pgf77
$ ~/examples/Pi/Pi-pgf77
 pi =    3.141592653589839
FORTRAN STOP
]]></screen>
</para>

<para>Запуск на кластере. Для запуска компилятора PGI на кластере удобно воспользоваться скриптом use-pgi-to.sh, которому в качестве аргумента передается любая команда, использующая компилятор. Например:
<screen><![CDATA[
$ ~/examples/use-pgi-to.sh pgf77 ~/examples/Pi/Pi.f -o ~/examples/Pi/Pi-pgf77-tp
17048.pbs-tp.hpc.cc.spbu.ru
$ cat job*17048
$ ~/examples/Pi/Pi-pgf77-tp
 pi =    3.141592653589839
FORTRAN STOP
]]></screen>
</para>

<para>Диагностика.
<screen><![CDATA[
$ ~/submit-tp -f ~/examples/pgi.sh 
17027.pbs-tp.hpc.cc.spbu.ru
$ cat job*o17027

pgcc 11.10-0 64-bit target on x86-64 Linux -tp core2 
Copyright 1989-2000, The Portland Group, Inc.  All Rights Reserved.
Copyright 2000-2011, STMicroelectronics, Inc.  All Rights Reserved.

pgCC 11.10-0 64-bit target on x86-64 Linux -tp core2 
Copyright 1989-2000, The Portland Group, Inc.  All Rights Reserved.
Copyright 2000-2011, STMicroelectronics, Inc.  All Rights Reserved.

pgfortran 11.10-0 64-bit target on x86-64 Linux -tp core2 
Copyright 1989-2000, The Portland Group, Inc.  All Rights Reserved.
Copyright 2000-2011, STMicroelectronics, Inc.  All Rights Reserved.

]]></screen>
</para>
</section>

<section id="ex_nag">
<title>Запуск компилятора <orgname>NAG</orgname></title>
<screen><![CDATA[
~/submit-tp -f ~/examples/test_nag.sh
]]></screen>
В результате вы получите информацию о версии компилятора. Содержимое скрипта <filename>~/examples/test_nag.sh</filename>:
<programlisting>
#!/bin/bash

# Specify licence server
export NAG_KUSARI_FILE=192.168.0.58:

# Check version
/usr/local/NAG/nagfor -V
</programlisting>
</section>

<section id="ex_nag_compiling">
<title>Компиляция с помощью <orgname>NAG</orgname></title>
Сначала необходимо указать сервер лицензий. Для этого нужно выполнить команду:
<screen><![CDATA[
export NAG_KUSARI_FILE=192.168.0.58:
]]></screen>
Затем можно запустить компиляцию (здесь она производится локально):
<screen><![CDATA[
/usr/local/NAG/nagfor ~/examples/Pi/Pi.f -o ~/NAG_pi
]]></screen>
Также можно использовать скрипт (который выполнит все эти действия). 
Ниже приведен пример запуска компиляции программы <filename>Pi.f</filename> (локально):
<screen><![CDATA[
~/examples/nag.sh ~/examples/Pi/Pi.f -o ~/NAG_pi 
]]></screen>
В результате будет создан файл <filename>~/NAG_pi</filename>.
Содержимое скрипта: <filename>~/examples/nag.sh</filename>:
<programlisting>
	#!/bin/bash

	# Specify licence server
	export NAG_KUSARI_FILE=192.168.0.58:

	# Start compilation
	/usr/local/NAG/nagfor $*
</programlisting>
Компиляция с поддержкой High Performance Fortran:
<screen><![CDATA[
~/examples/nag.sh -hpf ~/examples/Pi/Pi.f -o ~/NAG_pi
]]></screen>
Запуск компиляции на кластере:
<screen><![CDATA[
~/submit-tp -f "~/examples/nag.sh ~/examples/Pi/Pi.f -o ~/NAG_pi"
]]></screen>
Запуск скомпилированного приложения (локально):
<screen><![CDATA[
~/NAG_pi
]]></screen>
Запуск скомпилированного приложения на кластере:
<screen><![CDATA[
~/submit-tp -f ~/NAG_pi
]]></screen>
</section>

<section id="ex_compile-pi">
<title>Компиляция программы <command>Pi</command> в среде <command>openmpi</command></title>
<screen><![CDATA[
~/submit-tp -f ~/examples/Pi/Pi.comp-openmpi.sh -d ~/examples/Pi
]]></screen>
Содержимое скрипта <filename>~/examples/Pi/Pi.comp-openmpi.sh</filename>:
<programlisting>
#!/bin/sh
export prefix="/usr/lib64/openmpi/1.4-gcc"

export PATH=$prefix/bin:$PATH
export LD_LIBRARY_PATH=$prefix/lib:$LD_LIBRARY_PATH
mpif77 Pi.f -o Pi.openmpi
</programlisting>
</section>

<section id="ex_pi"><title>Запуск на 4-х ядрах программы <command>Pi</command> в среде <command>openmpi</command></title>
<screen><![CDATA[
$ ~/submit-tp -v openmpi -j Pi-6 -n 4 -f ~/examples/Pi/Pi.openmpi
2882.pbs-tp.hpc.cc.spbu.ru
]]></screen>
<para>
Просмотр всех очередей: 
</para>
<screen><![CDATA[
$ ~/status-tp -a

pbs-tp.hpc.cc.spbu.ru: 
                                                                         Req'd  Req'd   Elap
Job ID               Username Queue    Jobname          SessID NDS   TSK Memory Time  S Time
-------------------- -------- -------- ---------------- ------ ----- --- ------ ----- - -----
2882.pbs-tp.hpc.     zar      long-zar Pi-6                --      4  --    --    --  R   -- 
   node-20/0+node-19/0+node-18/0+node-17/0
]]></screen>

<para>
Результат:
</para>

<screen><![CDATA[
$ ll Pi-6.*
-rw------- 1 zar users   0 Jan 26 11:26 Pi-6.e2882
-rw------- 1 zar users 138 Jan 26 11:28 Pi-6.o2882
]]></screen>
</section>

<section id="ex_ff"><title>Запуск пакета <command>Firefly</command> с тестовым примером.</title>
<screen><![CDATA[
$ ~/submit-tp -d /usr/local/firefly/mpich2 -f /usr/local/firefly/mpich2/firefly \
-v mpich2 -j firefly -q short \
-i /usr/local/firefly/samples/condircbk.inp -o firefly.out -n 32
2861.pbs-tp.hpc.cc.spbu.ru
]]></screen>
Результат:
<screen><![CDATA[
$ ll firefly.*
-rw------- 1 viz users      0 Jan 25 18:02 firefly.e2861
-rw------- 1 viz users      0 Jan 25 18:02 firefly.o2861
-rw-r--r-- 1 viz users 992847 Jan 25 18:05 firefly.out
]]></screen>
<para>
В этом случае стандартного вывода нет &#160;&#8212; файл <filename>firefly.o2861</filename> пустой, 
но создан файл <filename>firefly.out</filename>, где <filename>firefly</filename> &#160;&#8212; это имя задачи, ключ <code language="bash">-j</code>.
</para>
</section>

<section id="ex_linpack"><title>Запуск теста <command>linpack</command> на 4 ядрах</title>
<screen><![CDATA[
~/submit-tp --input ~/linpack_10.3.3/benchmarks/mp_linpack/bin/intel64_mpich2_hydra/HPL.dat \
-n 4 --dir ~/linpack_10.3.3/benchmarks/mp_linpack/bin/intel64_mpich2_hydra \
-f ~/linpack_10.3.3/benchmarks/mp_linpack/bin/intel64_mpich2_hydra/xhpl -q infi -v mpich2 -j hplmpich2-4
]]></screen>
<para>
В этом случае создается файл в директории: <filename>~/linpack_10.3.3/benchmarks/mp_linpack/bin/intel64_mpich2</filename>,
имя которого было указано во входном файле <filename>~/linpack_10.3.3/benchmarks/mp_linpack/bin/intel64_mpich2/HPL.dat</filename>.
</para>
</section>

<section id="ex_stat"><title>Мониторинг загруженности кластера</title>
<screen><![CDATA[ 
$ ~/status-tp -p 
Total=368 
Free=352 
]]></screen> 
Здесь выводится полное кол-во доступных ядер <computeroutput>Total</computeroutput> и количество свободных ядер <computeroutput>Free</computeroutput>.
</section> 

<section id="ex_g03"><title>Запуск пакета <command>Gaussian</command> с тестовым примером</title>
<screen><![CDATA[
$ ~/submit-tp -n 4 -f ~/submit-g03.sh -i ~/test695.com -o ~/test695-n1.log
2983.pbs-tp.hpc.cc.spbu.ru
]]></screen>

Результат:
<screen><![CDATA[
-rw------- 1 viz users        0 Jan 28 13:18 job.1296209888.o2983
-rw------- 1 viz users       47 Jan 28 13:30 job.1296209888.e2983
-rw-r--r-- 1 viz users   966078 Jan 28 13:38 test695-n1.log
]]></screen>

В этом случае стандартного вывода нет – файл  <filename>job.1296209888.o2983</filename> пустой, но создан файл <filename>test695-n1.log</filename>.
</section> <!-- ex_g03 -->

<section id="ex_array-scilab"><title>Запуск массива задач в среде <command>scilab</command> с различным параметрами</title>
<screen><![CDATA[
$ ~/submit-tp -v pbs -q long -r 6-7 -f ./impkvart01.sh -j scilab-impedanec
3096.pbs-tp.hpc.cc.spbu.ru
$
]]></screen>

Исполняемый скрипт:
<screen><![CDATA[
$ cat impkvart01.sh 
#/bin/sh!
export PATH=$PATH:/usr/local/scilab-5.3.0/bin
worckdir=`mktemp -d`;
cd $worckdir

cat << EOF > startall 
exec('$HOME/mathlab/all09.03.sce');
stacksize('max');
rt=stacksize()
SQS03('$HOME/math/B23_0708.ts',200,1000,100,0.$PBS_ARRAYID);
tr=stacksize()
exit
EOF

scilab -nwni -f startall

mv  $worckdir $HOME/${PBS_JOBNAME}.outdir
echo $HOME/${PBS_JOBNAME}.outdir
]]></screen>

Рассмотрим исполняемый скрипт и объясним его содержание:
Объявление переменных окружения для исползуемого пакета <command>Scilab</command>.
<screen><![CDATA[
export PATH=$PATH:/usr/local/scilab-5.3.0/bin  
]]></screen>

Создание и преход в тестовую директорию (это будет происходить уже на рабочей ноде)
<screen><![CDATA[
worckdir=`mktemp -d`;
cd $worckdir
]]></screen>

Создание исполняемого скрипта <command>scilab</command>, с переменным параметром  <constant>$PBS_ARRAYID</constant>, котрый в нашем случае будет принимать значения 6 и 7.
<screen><![CDATA[
cat << EOF > startall 
exec('$HOME/mathlab/all09.03.sce');
stacksize('max');
rt=stacksize()
SQS03('$HOME/math/B23_0708.ts',200,1000,100,0.$PBS_ARRAYID);
tr=stacksize()
exit
EOF
]]></screen>

Запуск созданного скрипта
<screen><![CDATA[
scilab -nwni -f startall
]]></screen>

Копирование результатов и удаление их с рабочей ноды 
<screen><![CDATA[
mv  $worckdir $HOME/${PBS_JOBNAME}.outdir
]]></screen>

вывод в стандартный вывод имени каталога с результатом:
<screen><![CDATA[
echo $HOME/${PBS_JOBNAME}.outdir
]]></screen>

В итоге мы получаем 2 файла стандартного вывода:
<screen><![CDATA[
-rw------- 1 zar users   36 Feb  6 14:48 scilab-impedanec.o3096-6
-rw------- 1 zar users   36 Feb  6 14:47 scilab-impedanec.o3096-7
]]></screen>

В которых прописаны имена итоговых каталогов:
<screen><![CDATA[
$ cat scilab-impedanec.o3096-6
/home/zar/scilab-impedanec-6.outdir
$
]]></screen>

В итоговом каталоге мы видим результаты вычислений и сам сгенерированный скрипт, который запускался в <command>scilab</command>.
<screen><![CDATA[
$ ll /home/zar/scilab-impedanec-6.outdir
total 6750
-rw-r--r-- 1 zar users    4995 Feb  6 14:47 R357.1000.100
-rw-r--r-- 1 zar users     147 Feb  6 14:34 startall
-rw-r--r-- 1 zar users 1717355 Feb  6 14:47 XYSQS00-IM.357.1000.100
-rw-r--r-- 1 zar users 1717355 Feb  6 14:47 XYSQS00-RE.357.1000.100
-rw-r--r-- 1 zar users 1717355 Feb  6 14:47 YXSQS00-IM.357.1000.100
-rw-r--r-- 1 zar users 1717355 Feb  6 14:47 YXSQS00-RE.357.1000.100
]]></screen>
Содержимое файла <filename>/home/zar/scilab-impedanec-6.outdir/startall</filename>: 
<programlisting>
exec('/home/zar/mathlab/all09.03.sce');
stacksize('max');
rt=stacksize()
SQS03('/home/zar/math/B23_0708.ts',200,1000,100,0.6);
tr=stacksize()
exit
</programlisting>

Мы видим, что последний параметр, который у нас и был переменным в папке с префиксом <computeroutput>-6</computeroutput> и равен 0.6. В папке с префиксом <computeroutput>-7</computeroutput> &#160;&#8212; соответственно 0.7.
</section>

<section id="ex_ANSYS-on-virt">
<title>Запуск тестовой задачи ANSYS  на 1 ядре</title>
<screen><![CDATA[
$cd examples/ansys/
$ ll
total 3
-rw-r--r-- 1 zar users 1172 Nov 20 05:29 ansys_demo.inp
-rwxr-xr-x 1 zar users  231 Nov 20 06:30 run_ansys_tp.sh
$ ~/submit-tp -q virt -d `pwd` -f ./run_ansys_tp.sh -j `dirname \`pwd\`/. `
16019.pbs-tp.hpc.cc.spbu.ru
$ ll
total 2280
-rw-r--r-- 1 zar users    1742 Nov 20 06:43 ansys_demo.BCS
-rw-r--r-- 1 zar users 2031616 Nov 20 06:43 ansys_demo.db
-rw-r--r-- 1 zar users   65536 Nov 20 06:43 ansys_demo.emat
-rw-r--r-- 1 zar users      67 Nov 20 06:43 ansys_demo.err
-rw-r--r-- 1 zar users   65536 Nov 20 06:43 ansys_demo.esav
-rw-r--r-- 1 zar users   65536 Nov 20 06:43 ansys_demo.full
-rw-r--r-- 1 zar users    1172 Nov 20 05:29 ansys_demo.inp
-rw-r--r-- 1 zar users      76 Nov 20 06:43 ansys_demo.log
-rw-r--r-- 1 zar users     658 Nov 20 06:43 ansys_demo.mntr
-rw-r--r-- 1 zar users   15920 Nov 20 06:43 ansys_demo.out
-rw-r--r-- 1 zar users   65536 Nov 20 06:43 ansys_demo.rst
-rw------- 1 zar users       0 Nov 20 06:43 ansys.e16019
-rw------- 1 zar users       4 Nov 20 06:43 ansys.o16019
-rwxr-xr-x 1 zar users     231 Nov 20 06:30 run_ansys_tp.sh
-rw-r--r-- 1 zar users     272 Nov 20 06:43 vm1.vrt
$
]]></screen>
<para>
Где ansys_demo.out - выходной файл, ansys_demo.inp - исполняемый ansys файл.
В скрипте run_ansys_tp.sh следующие значения выжны:
</para>
Содержимое скрипта <filename>~/examples/ansys/run_ansys_tp.sh</filename>:
<programlisting>
#/bin/sh!
export ANSYSLMD_LICENSE_FILE="1055@v161.cc.spbu.ru"     #Параметры соединения с сервером лицензий.
export ANSYSLI_SERVERS="2325@v161.cc.spbu.ru"           #Параметры соединения с сервером лицензий.
export ANSYS130_DIR=/usr/local/ansys_inc/v130/ansys     #Путь к установленному на кластере пакету ANSYS
cd $PWD
$ANSYS130_DIR/bin/ansys130 -b nolist -p aa_r -j ansys_demo  -i ansys_demo.inp -o ansys_demo.out
</programlisting>
<para>
<computeroutput>-p aa_r </computeroutput> Лицензия.
Важно!! В примере использована просто  исследовательская лицензия. Если Ваша программа использует лицензию hpc (всегда, если задача распределенная)  - проставляйте параметр <computeroutput> -p aa_r_hpc  </computeroutput> или <computeroutput> -P aa_r_hpc </computeroutput>
(Может отличаться в зависимости от пакета).
</para>

При запуске используется очередь virt - на данный момент оптимизированная очередь н авиртуальном кластере для работы с пакетом ANSYS. В случае оптимизации для ANSYS ТП кластере будет размещено объявление на сайте ПТЦ.


</section><!--ex_ANSYS-on-virt-->


<section id="ex_qdel"><title>Мониторинг и удаление задания</title>

Видим в очереди задание, <computeroutput>Job Id</computeroutput> = <quote>имя задания</quote>
<screen><![CDATA[
$ ~/status-tp -a

pbs-tp.hpc.cc.spbu.ru: 
                                                                         Req'd  Req'd   Elap
Job ID               Username Queue    Jobname          SessID NDS   TSK Memory Time  S Time
-------------------- -------- -------- ---------------- ------ ----- --- ------ ----- - -----
3097-5.pbs-tp.hp     zar      long-304 scilab-impedanec   2364     1  --    --    --  R 00:03
   node-ib-42/0
]]></screen>

Смотрим подробные  характеристики задания
<screen><![CDATA[
$ ~/status-tp -f 3097.pbs-tp.hpc.cc.spbu.ru
qstat: Unknown Job Id 3097.pbs-tp.hpc.cc.spbu.ru
Job Id: 3097-5.pbs-tp.hpc.cc.spbu.ru
    Job_Name = scilab-impedanec-5
    Job_Owner = zar@alice24.spbu.ru
    resources_used.cput = 00:02:58
    resources_used.mem = 38336kb
    resources_used.vmem = 2316676kb
    resources_used.walltime = 00:02:59
    job_state = R
    queue = long-3046
    server = pbs-tp.hpc.cc.spbu.ru
    Checkpoint = u
    ctime = Sun Feb  6 19:25:17 2011
    Error_Path = alice24.spbu.ru:/home/zar/scilab-impedanec.e3097-5
    exec_host = node-ib-42/0
    Join_Path = n
    Keep_Files = n
    Mail_Points = a
    mtime = Sun Feb  6 19:25:18 2011
    Output_Path = alice24.spbu.ru:/home/zar/scilab-impedanec.o3097-5
    Priority = 0
    qtime = Sun Feb  6 19:25:18 2011
    Rerunable = True
    Resource_List.nodect = 1
    Resource_List.nodes = 1:ppn=1
    session_id = 2364
    Variable_List = PBS_O_HOME=/home/zar,PBS_O_LANG=en_US.UTF-8,
        PBS_O_LOGNAME=zar,
        PBS_O_PATH=/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin:/home/zar/b
        in,PBS_O_MAIL=/var/spool/mail/zar,PBS_O_SHELL=/bin/bash,
        PBS_O_HOST=alice24.spbu.ru,PBS_SERVER=localhost,
        PBS_O_WORKDIR=/home/zar,PBS_O_QUEUE=long-3046,PBS_ARRAYID=5
    comment = Job started on Sun Feb 06 at 19:25
    etime = Sun Feb  6 19:25:18 2011
    submit_args = -t 5 -N scilab-impedanec -l nodes=1:ppn=1 -q long-3046@pbs-t
        p.hpc.cc.spbu.ru file1297009516.sh
    job_array_id = 5
    job_array_request = 5
    start_time = Sun Feb  6 19:25:18 2011
    start_count = 1
]]></screen>

Удаляем задание:
<screen><![CDATA[
$ qdel 3097.pbs-tp.hpc.cc.spbu.ru@pbs-tp.hpc.cc.spbu.ru
]]></screen>

Так же можно воспользоваться скриптом <filename>~/qdel</filename>
<screen><![CDATA[
$ ~/qdel 3097
]]></screen>

Проверяем удаление:
<screen><![CDATA[
$ ~/status-tp -f 3097.pbs-tp.hpc.cc.spbu.ru
qstat: Unknown Job Id 3097.pbs-tp.hpc.cc.spbu.ru
$
]]></screen>
</section>

<section id="ex_abinit"><title>Запуск тестовой задачи <command>abinit</command> на 24-х ядрах</title>
<screen><![CDATA[
$ cd examples/
$ cd abinit01/
$ ll
total 1
-rw-r--r-- 1 zar users 167 Aug 24 15:22 tparal_1.files
$ ~/submit-abinit.sh ./tparal_1.files 24 ./log long
10876.pbs-tp.hpc.cc.spbu.ru
$ ~/status-tp -a

pbs-tp.hpc.cc.spbu.ru:
                                                                        Req'd  Req'd   Elap
Job ID               Username Queue    Jobname          SessID NDS   TSK Memory Time  S Time
-------------------- -------- -------- ---------------- ------ ----- --- ------ ----- - -----
10876.pbs-tp.hpc     zar      long     abinit.131418499    --     24  --    --  48:00 R   --
  node-ib-47/7+node-ib-47/6+node-ib-47/5+node-ib-47/4+node-ib-47/3
  +node-ib-47/2+node-ib-47/1+node-ib-47/0+node-ib-46/7+node-ib-46/6
  +node-ib-46/5+node-ib-46/4+node-ib-46/3+node-ib-46/2+node-ib-46/1
  +node-ib-46/0+node-ib-32/7+node-ib-32/6+node-ib-32/5+node-ib-32/4
  +node-ib-32/3+node-ib-32/2+node-ib-32/1+node-ib-32/0

]]></screen>
<para>
Среди созданных фалов будут созданы следующие:
<filename>tparal_1.out</filename> - результат
<filename>log</filename>  - аутпут работы команды по которому Вы можете следить за работой в процессе.
Обязательным параметром является только первый  - с перечислением входных и выходных файлов. Кол-во ядер по умолчанию - 1 , очередь - long, лог пишеться в аутпут (<filename>abinit.1314184998.o10876</filename>)
</para>
<para>
Сам собранный пакет расположен
<filename>/usr/local/abinit-6.8.1/</filename>
Исполняемый файл
<filename>/usr/local/abinit-6.8.1/bin/abinit</filename>
</para>
<para>
Параллельную версию следует использовать с mpich2 , собранный с ntel® Composer XE 2011. Поэтому, если Вы будете самостоятельно собирать скрипты для  abint  - используйте пути:

<constant>MPICH2_PREFIX=/usr/local/mpich2-1.4.INT.20110813/</constant>
<constant>UNTEL_PREFIX=/usr/local/intel</constant>
</para>
</section> <!-- ex_abinit -->

<section id="ex_crystal"><title>Запуск пакета <command>Crystal</command> с тестовым примером</title>
Входной файл необходимо поместить в директорию <filename>~/crystal/inputs</filename>. В качестве теста используется файл <filename>~/examples/crystal/tio2pr.d12</filename>
<screen><![CDATA[
$ mkdir -p ~/crystal/inputs
$ cp ~/examples/crystal/tio2pr.d12 ~/crystal/inputs
$ ~/submit-tp -q short -n 24 -f ~/submit-crystal.sh -j cr09_tio2pr-n4 -i tio2pr
18216.pbs-tp.hpc.cc.spbu.ru
]]></screen>
В директории, где запускался скрипт, по окончании вычисления будут располагаться файлы:
<screen><![CDATA[
$ ls -l *18216
-rw------- 1 vij users  493 Dec 19 22:12 cr09_tio2pr-n4.e18216
-rw------- 1 vij users 1285 Dec 19 22:12 cr09_tio2pr-n4.o18216
]]></screen>

Результат будет располагаться в директории <filename>~/crystal/tio2pr.20111219-221158</filename>. Название её образовывается из имени входного файла, даты и времени запуска задачи:
<screen><![CDATA[
$ ls -l ~/crystal/tio2pr.20111219-221158
total 680
-rw-r--r-- 1 vij users    264 Dec 19 22:11 machines.LINUX
-rw-r--r-- 1 vij users     33 Dec 19 22:11 nodes.par
-rw-r--r-- 1 vij users    455 Dec 19 22:11 tio2pr.d12
-rw-r--r-- 1 vij users 172428 Dec 19 22:12 tio2pr.f9
-rw-r--r-- 1 vij users 442793 Dec 19 22:12 tio2pr.f98
-rw-r--r-- 1 vij users  58837 Dec 19 22:12 tio2pr.out
]]></screen>

В этом случае стандартного вывода нет – файл  <filename>job.1296209888.o2983</filename> пустой, но создан файл <filename>test695-n1.log</filename>.
</section> <!-- ex_crystal -->

<section id="ex_wien2k"><title>Запуск пакета <command>WIEN2k</command></title>
Последняя версия WIEN2k находится в директории <filename>/usr/local/WIEN2k/WIEN2k_11</filename> .
Для запуска WIEN2k, нужно задать некоторые переменные окружения. 
Есть два способа сделать это.
Первый способ - с использованием userconfig_lapw. Для этого выполните следующую команду (нужно будет ответить на несколько вопросов):
<screen><![CDATA[
/usr/local/WIEN2k/WIEN2k_11/userconfig_lapw
]]></screen>
Скрипт изменит файл <filename>~/.bashrc</filename> нужным образом.
После чего потребуется дописать следующие строчки в <filename>~/.bashrc</filename>:
<screen><![CDATA[
. /usr/local/intel/mkl/bin/mklvars.sh intel64
. /usr/local/intel/bin/compilervars.sh intel64
]]></screen>

Второй способ - использование файла <filename>~/.bashrc.wien2k</filename>.
В этом файле указан минимум переменных для работы.
Содержимое файла <filename>~/.bashrc.wien2k</filename>:
<screen><![CDATA[
# Intel MKL
. /usr/local/intel/mkl/bin/mklvars.sh intel64
. /usr/local/intel/bin/compilervars.sh intel64

export WIENROOT=/usr/local/WIEN2k/WIEN2k_11
export STRUCTEDIT_PATH=$WIENROOT/SRC_structeditor/bin
export OCTAVE_EXEC_PATH=${PATH}::    
export OCTAVE_PATH=${STRUCTEDIT_PATH}::
export PATH=$PATH:$WIENROOT:$STRUCTEDIT_PATH:.
ulimit -s unlimited
]]></screen>
Перед запуском WIEN2k нужно добавить следующую строчку в конец файла <filename>~/.bashrc</filename>:
<screen><![CDATA[
source ~/.bashrc.wien2k
]]></screen>

Для запуска задачи на кластере нужно указать директорию, в которой хранятся данные для расчетов.
Запуск задачи на кластере осуществляется следующим образом (предполагается, что в директории <filename>~/WIEN2k/myProject</filename> хранятся данные для расчетов):
<screen><![CDATA[
~/submit-tp -f "~/submit-wien2k.sh ~/WIEN2k/myProject"
]]></screen>
При этом результаты расчетов будут находится в указанной папке (<filename>~/WIEN2k/myProject</filename> для этого примера).
Для запуска задачи на нескольких ядрах, выполните следующую команду:
<screen><![CDATA[
~/submit-tp -n <число_ядер> -f "~/submit-wien2k.sh ~/WIEN2k/myProject"
]]></screen>
Внимание, файл <filename>.machines</filename> в директории с данными будет переписан (в нем будут указаны задействованные узлы).
Содержимое скрипта <filename>~/submit-wien2k.sh</filename>:
<screen><![CDATA[
!/bin/bash

if [ -n "$1" ]; then
 
  SPECIFIED_DIR="$1"
 
  if [ ! -d /tmp/$USER ]; then
    mkdir /tmp/$USER
  fi
  if [ ! -d /tmp/$USER/wien2k ]; then
    mkdir /tmp/$USER/wien2k
  fi

  # Create and export SCRATCH directory
  export SCRATCH=`mktemp -d -p /tmp/$USER/wien2k/` 

  # Change dir to project dir
  cd "$SPECIFIED_DIR"
  
  # Create .machines file
  cat $PBS_NODEFILE > machines.LINUX
  cat machines.LINUX |uniq > nodes.par
  for i in `cat nodes.par | xargs`; do n=$(grep $i machines.LINUX|wc -l); echo 1:${i}:$n; done > .machines

  # Create SCRATCH dir on each node
  for n in `cat nodes.par`; do ssh $n "mkdir -p $SCRATCH;"; done  
  
  echo "Nodes:"
  cat .machines
  echo "Using WIEN2k: $WIENROOT"

  # Start calculations
  time $WIENROOT/run_lapw -p -NI -ec 0.0001

  # Remove SCRATCH dir
  for n in `cat nodes.par`; do ssh $n "rm -rf $SCRATCH"; done  

else
  echo "Please specify a working directory!"
fi
]]></screen>
Для задания других параметров, либо запуска другой команды WIEN2k измените следующую строчку в скрипте <filename>~/submit-wien2k.sh</filename>:
<screen><![CDATA[
time $WIENROOT/run_lapw -p -NI -ec 0.0001
]]></screen>
Простой пример запуска задачи на счёт приведён ниже:
<screen><![CDATA[
$ mkdir -p ~/WIEN2k
$ cd ~/WIEN2k
$ tar xvf ~/examples/WIEN2k/Fe.tar.gz 
$ ~/submit-tp -n 16 -f "~/submit-wien2k.sh ~/WIEN2k/Fe"
19089.pbs-tp.hpc.cc.spbu.ru
]]></screen>
В директории, где запускался скрипт, по окончании вычисления будут располагаться файлы:
<screen><![CDATA[
$ ls -l *19089
-rw------- 1 vij users 648 Dec 27  2011 job.1324937055.e19089
-rw------- 1 vij users 116 Dec 27  2011 job.1324937055.o19089
]]></screen>

Результат вычислений будет располагаться в директории <filename>~/WIEN2k/Fe</filename>.
Стоит также упомянуть о проблеме, связанной с интерактивным вводом.
Для задач, требующих участия пользователя (интерактивного ввода пользовательских данных)
можно использовать следующее решение. Для передачи таких данных программе в скрипте
небходимо написать код вида:
<screen><![CDATA[
echo -e "<параметр 1>\n <параметр 2>\n ... <параметр k>\n" | <программа>
]]></screen>
При этом i-ый параметр соответствует i-ому преждложению ввода от программы.
Поясним это на примере. В пакете WIEN2k есть приложение kgen, которое удобно запускать
через программу x (получается x kgen). Ему требуется получить от пользователя 2 значения.
Вначале появляется предложение ввести данные. После того, как пользователь ввел данные,
приложение обрабатывает их и через некоторое время от пользователя снова требуется ввод данных.
Таким образом, приложение дважды задаст пользователю вопрос и потребует ввода данных.
Но как быть, если приложение запускается на кластере, где пользовательский ввод невозможен?
Решение - указанная выше конструкция. В данном случае она будет выглядеть так 
(вместо "3" и "0" могут быть любые другие значения):
<screen><![CDATA[
echo -e "3\n 0\n" | x kgen
]]></screen>
Таким образом, на первый запрос приложению будет передано значение "3", а на второй - "0".
</section> <!-- ex_wien2k -->

<section id="ex_molpro"><title>Запуск пакета <command>Molpro</command></title>
Последовательная версия находится в директории <filename>/usr/local/molpro/molpros_2010_1_Linux_x86_64_i8</filename> .
Параллельная версия - в директории <filename>/usr/local/molpro/molprop_2010_1_Linux_x86_64_i8</filename> .
В качесте теста используется файл <filename>h2.com</filename>. Для запуска последовательной версии нужно выполнить команду:
<screen><![CDATA[
/usr/local/molpro/molpros_2010_1_Linux_x86_64_i8/bin/molpro ~/examples/molpro/h2.com
]]></screen>
Запуск параллельной версии на кластере. Прежде всего нужно добавить следующую строчку
в файл <filename>~/.bashrc</filename>:
<screen><![CDATA[
source ~/.bashrc.molpro
]]></screen>
Содержимое файла <filename>~/.bashrc.molpro</filename>:
<screen><![CDATA[
export TCGRSH=/usr/bin/ssh
]]></screen>
Простой пример запуска приведен ниже:
<screen><![CDATA[
mkdir ~/Molpro
cp ~/examples/molpro/h2.com ~/Molpro/
~/submit-tp -n 4 -f "~/submit-molpro.sh ~/Molpro/h2.com"
]]></screen>
Содержимое файла <filename>~/submit-molpro.sh</filename>:
<screen><![CDATA[
#!/bin/bash

SPECIFIED_FILE="$1"
MOLPRO_DIR=/usr/local/molpro/molprop_2010_1_Linux_x86_64_i8/bin/
time $MOLPRO_DIR/molpro $SPECIFIED_FILE
]]></screen>
В директории, где запускался скрипт, по окончании вычисления будут располагаться файлы: 
<screen><![CDATA[
[gajduchok@v339 ~]$ ls -l job.1328546324.*
-rw------- 1 gajduchok users  43 Feb  6 20:39 job.1328546324.e21459
-rw------- 1 gajduchok users 716 Feb  6 20:39 job.1328546324.o21459
]]></screen>
Результаты вычислений для данного примера будут располагаться в директории 
<filename>~/Molpro/</filename> в файле <filename>h2.out</filename> 
(либо <filename>h2.out_*</filename> для последующих запусков, где вместо "*"
будет стоять число).
</section> <!-- ex_molpro -->

<section id="ex_openfoam"><title>Запуск пакета <command>OpenFOAM</command></title>
Последняя версия OpenFOAM расположена в <filename>/usr/local/OpenFOAM/openfoam</filename> .
Последняя версия third party software для OpenFOAM - в <filename>/usr/local/OpenFOAM/thirdparty</filename> 
(в первую очередь стоит упомянуть ParaView). Расчеты могут выполняться как последовательно,
так и параллельно (с использованием MPI). Для начала работы с OpenFOAM нужно добавить
следующую строчку в файл <filename>~/.bashrc</filename>:
<screen><![CDATA[
source ~/.bashrc.openfoam
]]></screen>
Содержимое файла <filename>~/.bashrc.openfoam</filename>:
<screen><![CDATA[
export FOAM_INST_DIR=/usr/local/OpenFOAM
source ${FOAM_INST_DIR}/openfoam/etc/bashrc
# For Qt libs
export LD_LIBRARY_PATH=${FOAM_INST_DIR}/qt/lib/:$LD_LIBRARY_PATH
# Use this version of MPI for OpenFOAM
export PATH=${FOAM_INST_DIR}/mpi/bin/:$PATH
# For Intel libraries (compiled with Intel compilers)
source /usr/local/intel/mkl/bin/mklvars.sh intel64
source /usr/local/intel/bin/compilervars.sh intel64
]]></screen>
Теперь можно перейти к примеру. Тестовые задачи расположены в папке 
<filename>/usr/local/OpenFOAM/OpenFOAM-2.1.0/tutorials</filename> . Рассмотрим задачу - 
расчет течения жидкости в каверне. Данные для задачи хранятся в директории 
<filename>/usr/local/OpenFOAM/OpenFOAM-2.1.0/tutorials/incompressible/icoFoam/cavity</filename> .
Скопируем данный каталог в директорию <filename>~/examples/openfoam/</filename> для проведения расчетов:
<screen><![CDATA[
mkdir ~/examples/openfoam/
cp -r /usr/local/OpenFOAM/OpenFOAM-2.1.0/tutorials/incompressible/icoFoam/cavity ~/examples/openfoam/
]]></screen>
Теперь перейдем в папку <filename>~/examples/openfoam/cavity/</filename> . До конца этого примера мы будем работать в ней:
<screen><![CDATA[
cd ~/examples/openfoam/cavity/
]]></screen>
Перед расчетом необходимо выполнить команду blockMesh для формирования сетки.
Для проверки можно использовать команду checkMesh.
<screen><![CDATA[
blockMesh
checkMesh
]]></screen>
Все. Теперь осталось запустить расчет при помощи команды icoFoam:
<screen><![CDATA[
icoFoam
]]></screen>
На самом деле OpenFOAM содержит множесто программ (solvers) для различных расчетов.
Так, в следующем примере мы будем использовать программу pisoFoam.
Теперь рассмотрим запуск расчета на кластере. Мы возьмем пример pitzDaily:
<screen><![CDATA[
cp -r /usr/local/OpenFOAM/OpenFOAM-2.1.0/tutorials/incompressible/pisoFoam/les/pitzDaily/ ~/examples/openfoam/
cd ~/examples/openfoam/pitzDaily/
]]></screen>
Запускаем blockMesh для формирования сетки. 
И снова мы можем проверить результат при помощи команды checkMesh.
<screen><![CDATA[
blockMesh
checkMesh
]]></screen>
Если все получилось удачно (Mesh OK), идем дальше. Теперь нужно произвести декомпозицию задачи.
Для этого нам нужно создать файл <filename>decomposeParDict</filename> в папке
<filename>system</filename> (<filename>~/examples/openfoam/pitzDaily/system/</filename>).
Этот файл содержит сведения о том как именно разбить задачу для параллельного выполнения.
Мы возьмем уже существующий файл из примера dambreak:
<screen><![CDATA[
cp /usr/local/OpenFOAM/OpenFOAM-2.1.0/tutorials/multiphase/interFoam/laminar/damBreak/system/decomposeParDict ~/examples/openfoam/pitzDaily/system/
]]></screen>
Теперь запустим собственно разбиение задачи (в данном примере на 4 части):
<screen><![CDATA[
decomposePar
]]></screen>
После этого мы готовы запустить расчет на кластере. Для этого используем скрипты 
<filename>~/submit-tp</filename> и <filename>~/submit-openfoam.sh</filename>
(непосредственно для расчетов используется программа pisoFoam):
<screen><![CDATA[
~/submit-tp -n 4 -f "~/submit-openfoam.sh ~/examples/openfoam/pitzDaily/ pisoFoam"
]]></screen>
Для скрипта <filename>~/submit-openfoam.sh</filename> сначала указывается директория с данными, затем команда (solver) с опциями,
которая будет выпонять вычисления. Опцию "-parallel" указывать не нужно - она будет добавлена при запуске расчетов.
Указанная команда запустит счет на 4 ядрах. Обратите внимание, что задействованных ядер
должно быть ровно числу участков, на которые поделена задача при помощи decomposePar 
(в нашем случае их 4, это указывается в файле decomposeParDict). 
Расчет завершится ошибкой, если указать другое число ядер (как меньшее, так и большее).
Содержимое <filename>~/submit-openfoam.sh</filename>:
<screen><![CDATA[
#!/bin/bash

DIRECTORY="$1"
shift
COMMAND_WITH_OPTIONS="$*"

cd $DIRECTORY
NP=`cat $PBS_NODEFILE | wc -l`
time ${FOAM_INST_DIR}/mpi/bin/mpirun --hostfile $PBS_NODEFILE -np $NP $COMMAND_WITH_OPTIONS -parallel
]]></screen>	
В директории, где запускался скрипт, по окончании вычисления будут располагаться файлы (примерные названия): 
<screen><![CDATA[
[gajduchok@v339 pitzDaily]$ ls -l job*   
-rw------- 1 gajduchok users     44 Mar 19 17:00 job.1332161937.e24146
-rw------- 1 gajduchok users 896892 Mar 19 17:00 job.1332161937.o24146
]]></screen>
В них содержится стандартный вывод и ошибки (если есть), выданные в процессе вычислений.
Результат вычислений будет располагаться в директории <filename>~/examples/openfoam/pitzDaily/</filename> .
Обратимся к нему. Сперва необходимо объединить результаты, полученные
на отдельных ядрах. Для этого вызовем команду:
<screen><![CDATA[
reconstructPar
]]></screen>
Вот и все. Теперь можно просмотреть результат. Визуализацией занимается программа ParaView.
Для ее вызова в OpenFOAM есть стандратный скрипт - paraFoam. Его и вызываем:
<screen><![CDATA[
paraFoam
]]></screen>
Появится окно программы paraView (если вы используете ssh, не забудьте про опцию "-X"
при подключении к своей машине для запуска графических приложений: ssh user@hostname -X).
Нажмите на кнопку "Apply" во вкладке "Properties" панлеи "Object Inspector" (слева внизу).
Затем перейдите во вкладку "Display" той же панели и выберите из списка "Color by" то, что 
необходимо отображать. Например, "U". Для просмотра движения жидкости в трубе нажмите
кнопку "Play". Для перехода к конкретному моменту времени можно воспользоваться панелью 
"Time" (справа вверху) и задать нужный момент (один из тех, для которых проводился расчет). 
</section> <!-- ex_openfoam -->

<section id="ex_concorde"><title>Запуск пакета <command>Concorde</command></title>
Последняя скомпилированная версия Concorde расположена в <filename>/usr/local/Concorde/concorde</filename> .
Есть также прекомпилированная версия, расположенная в <filename>/usr/local/Concorde/binaries</filename> .
Здесь будет рассмотрен пример запуска на кластере нескольких независимых вычислений Concorde:
<screen><![CDATA[
~/submit-tp -n 8 -f ~/submit-concorde.sh 
]]></screen>
Эта команда запустит 8 независимых расчетов на 8 ядрах.
Содержимое скрипта <filename>~/submit-concorde.sh</filename>:
<screen><![CDATA[
#!/bin/bash

i=1
for node in `cat $PBS_NODEFILE`; do
  echo node=$node i=$i
  ssh $node "~/examples/concorde-script.sh $i" &
  let i=i+1
done
wait
]]></screen>
Как видно, в цикле запускается скрипт <filename>concorde-script.sh</filename> на каждом из выделенных узлов (по ssh).
При этом число запусков скрипта на узле соответствует числу выделенных на нем ядер (все выделенные ядра используются).
Задача не завершится, пока хотя бы один процесс не завершил расчет. Для этого используется команда wait.
Таким образом, все выделенные ядра задействованы, но их вычисления независимы.
Это видно по тексту скрипта <filename>concorde-script.sh</filename>:
<screen><![CDATA[
#!/bin/bash

NUM="$1"
if [ ! -d ~/examples/concorde/$NUM ]; then
  mkdir -p ~/examples/concorde/$NUM;
fi
cd ~/examples/concorde/$NUM
# Start calculations
time /usr/local/Concorde/concorde/TSP/concorde -s 99 -k $((NUM * 100))
]]></screen>
Этот скрипт является просто тестом для проверки работы Concorde.
Он создает директорию в ~/examples/concorde/, переходит туда и начинает вычисления
(результаты будут храниться в созданной директории).
После выполнения задачи в текущей директории появятся файлы (примерные имена):
<screen><![CDATA[
[gajduchok@v339 ~]$ ls -l job.1332447269.*
-rw------- 1 gajduchok users  170 Mar 23 00:15 job.1332447269.e24279
-rw------- 1 gajduchok users 6205 Mar 23 00:15 job.1332447269.o24279
]]></screen>
В них содержится стандартный вывод и ошибки (если есть), выданные в процессе вычислений.
</section> <!-- ex_concorde -->

<section id="ex_gromacs"><title>Запуск пакета <command>GROMACS</command></title>
Последняя версия пакета GROMACS расположена в <filename>/usr/local/GROMACS/gromacs</filename> .
Расчеты могут выполняться как последовательно, так и параллельно (с использованием MPI). 
Для начала работы с GROMACS нужно добавить следующую строчку в файл <filename>~/.bashrc</filename>:
<screen><![CDATA[
source ~/.bashrc.gromacs
]]></screen>
Содержимое файла <filename>~/.bashrc.gromacs</filename> :
<screen><![CDATA[
source /usr/local/GROMACS/gromacs/bin/GMXRC.bash
# For Intel libraries (compiled with Intel compilers)
source /usr/local/intel/mkl/bin/mklvars.sh intel64
source /usr/local/intel/bin/compilervars.sh intel64
]]></screen>
Теперь рассмотрим пример. Тестовые задачи расположены в папке 
<filename>/usr/local/GROMACS/gromacs/share/gromacs/tutor</filename> .
Создадим папку <filename>~/examples/gromacs</filename> , затем скопируем пример "water" 
в нее и перейдем в эту папку для выполнения расчетов:
<screen><![CDATA[
mkdir -p ~/examples/gromacs
cp -r /usr/local/GROMACS/gromacs/share/gromacs/tutor/water/ ~/examples/gromacs/
cd ~/examples/gromacs/water/
]]></screen>
Данный пример довольно простой - он нужен лишь для первого знакомства с GROMACS.
Вначале рассмотрим запуск расчетов на виртуальной машине, а затем на кластере.
Все необходимые файлы уже есть в папке "water". Поэтому можно запустить программу 
grompp для создания файла с раширением *.tpr . Этот файл будет входным файлом для
программы mdrun. Просто выполняем (опция "-maxwarn" указывает максимальное число предупреждений,
"-v" задает verbose-режим для подробного вывода):
<screen><![CDATA[
grompp -v -maxwarn 5
]]></screen>
Все. Теперь можно запускать симуляцию. Для запуска на виртуальной машине выполните:
<screen><![CDATA[
mdrun -v
]]></screen>
После выполнения mdrun можно запустить программу ngmx для визуализации:
<screen><![CDATA[
ngmx
]]></screen>
Для запуска на кластере сначала также выполняем grompp, при этом на виртуальной машине.
Нет смысла запускать программу grompp на кластере, так как лишь программа mdrun_mpi 
использует параллельные вычисления. 
Все остальные программы пакета GROMACS (в том числе и grompp) 
не имеют средств распараллеливания. Поэтому выполняем:
<screen><![CDATA[
cd ~/examples/gromacs/water/
grompp -v -maxwarn 5
]]></screen>
Теперь можно запустить симуляцию на кластере:
<screen><![CDATA[
~/submit-tp -n 8 -f "~/submit-gromacs.sh ~/examples/gromacs/water/ mdrun_mpi"
]]></screen>
Где "-n" указывает число используемых ядер, "-f" указывает команду для исполнения.
Мы запускаем скрипт <filename>~/submit-gromacs.sh</filename> . Ему передаются следующие
аргументы: вначале идет полный путь к папке с данными 
(<filename>~/examples/gromacs/water/</filename> в нашем случае), затем исполняемая
программа (mdrun_mpi), потом могут идти опции для этой программы (например, "-s topol.tpr").
Обратите внимание, что для запуска на кластере используется только программа mdrun_mpi,
обычный mdrun на кластере запускаться не будет! Содержимое скрипта <filename>~/submit-gromacs.sh</filename> :
<screen><![CDATA[
#!/bin/bash

WORK_DIR="$1"
shift
COMMAND_WITH_OPTIONS="$*"

cd ${WORK_DIR}
NP=`cat $PBS_NODEFILE | wc -l`

time /usr/local/mpich2/bin/mpirun -machinefile $PBS_NODEFILE -np $NP ${COMMAND_WITH_OPTIONS}
]]></screen>
В директории, где запускался скрипт, по окончании вычисления будут располагаться файлы (примерные названия): 
<screen><![CDATA[
[gajduchok@v339 water]$ ls -l job*
-rw------- 1 gajduchok users 6650 Apr 16 11:43 job.1334562175.e25423
-rw------- 1 gajduchok users 7711 Apr 16 11:43 job.1334562175.o25423
]]></screen>
В них содержится стандартный вывод и ошибки (если есть), выданные в процессе вычислений.
Результат вычислений будет располагаться в директории <filename>~/examples/gromacs/water</filename> .
Для визуализации также можно запустить ngmx на Вашей виртуальной машине.
Теперь стоит рассказать о checkpoints, используемых в GROMACS. Они позволяют сохранять
текущее состояние симуляции, чтобы продолжить симуляцию с последнего сохраненного этапа,
если произойдет какой-нибудь сбой.
Без использования checkpoints GROMACS начнет симуляцию "с нуля" в случае остановки задания
и его последующего перезапуска (checkpoints были введены для случая непредвиденной остановки
расчетов по каким-либо внешним причинам).
С использованием checkpoints GROMACS продолжит симуляцию, используя последнее сохраненное состояние.
Чтобы запустить задачу на кластере с использованием checkpoints, выполняем:
<screen><![CDATA[
~/submit-tp -n 8 -f "submit-gromacs.sh ~/examples/gromacs/water/ mdrun_mpi -cpo checkPoint.cpt"
]]></screen>
Опция "-cpo" указывает файл для сохранения текущего состояния симуляции.
Также стоит упомянуть об опции "-cpt", которая позволяет задавать интервал, через который
GROMACS будет сохранять состояние вычислений (по умолчанию 15 минут):
<screen><![CDATA[
~/submit-tp -n 8 -f "submit-gromacs.sh ~/examples/gromacs/water/ mdrun_mpi -cpt 1 -cpo checkPoint.cpt"
]]></screen>
Для опции "-cpt" временной интервал задается в минутах. В указанном выше случае этот интервал
равен 1 минуте (интервал, через который GROMACS будет сохранять состояние симуляции).
Для перезапуска задания с использованием checkpoints выполняем:
<screen><![CDATA[
~/submit-tp -n 8 -f "submit-gromacs.sh ~/examples/gromacs/water/ mdrun_mpi -cpi checkPoint.cpt"
]]></screen>
Опция "-cpi" указывает файл, в который записывались checkpoints. 
Теперь GROMACS продолжит расчет с момента последнего сохранения.
Результаты вычислений также будут находится в папке <filename>~/examples/gromacs/water</filename> 
(для данного примера).
</section> <!-- ex_gromacs -->

</section> <!-- Примеры запуска задач -->
</section> <!-- Запуск задач -->

<section id="soft"><title>Программное обеспечение</title>

<section id="soft_os"><title>Операционная система</title>
На вычислительных узлах установлен <!-- SUSE Linux Enterprise Server 10.1 --> <command>CentOS release 5.6</command>.
</section> <!-- Операционная система -->

<section id="soft_queue"><title>Очереди</title>

<section id="soft_queue_torque"><title>Torque</title>
<command>TORQUE</command> (англ. Terascale Open-Source Resource and QUEue Manager) — менеджер распределенных ресурсов для 
вычислительных кластеров из машин под управлением Linux и других Unix-подобных операционных систем, 
одна из современных версий Portable Batch System (PBS). Распространяется под свободной лицензией 
OpenPBS Software License. <command>TORQUE</command> разрабатывается и поддерживается сообществом на базе проекта <command>OpenPBS</command>. Для менеджера существует более 1200 патчей и расширений, написанных крупнейшими организациями и лабораториями, среди которых US DOE, USC, PNLL и др., это позволяет достичь высокой степени масштабируемости и отказоустойчивости менеджера как системы.

Основная функция <command>TORQUE</command> &#160;&#8212; распределение вычислительных задач среди доступных вычислительных ресурсов. 
<command>TORQUE</command> содержит собственный планировщик заданий, определяющий момент запуска задач. Аналогом <command>TORQUE</command> являются система Cleo, а также другие версии Portable Batch System. Существует также сторонний планировщик заданий <command>Maui</command>, 
который обладает значительно большей функциональностью по сравнению со стандартным, и, поэтому, часто 
используется совместно с <command>TORQUE</command>. Возможна интеграция <command>TORQUE</command> с <command>OpenMP</command> и <command>MPICH</command>.
<itemizedlist>
 <listitem> Версия: 2.4.8 </listitem>
 <listitem> Расположение: <filename>/usr/bin</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.clusterresources.com/products/torque-resource-manager.php">WWW</ulink></listitem>
</itemizedlist>

</section> <!-- Torque -->

<section id="soft_queue_maui"><title>Maui</title>
<command>Maui cluster scheduler</command> &#160;&#8212; планировщик заданий в параллельных и распределенных вычислительных системах (кластерах).
Как правило используется совместно с менеджером распределенных ресурсов <command>TORQUE</command>.

Maui позволяет выбирать различные политики планирования, поддерживает динамическое изменение приоритетов, исключения.
Все это улучшает управляемость и эффективность машин, начиная от простых кластеров до суперкомпьютеров.
<itemizedlist>
 <listitem> Версия: 3.3.1 </listitem>
 <listitem> Расположение: <filename>/usr/local</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.clusterresources.com/products/maui-cluster-scheduler.php">WWW</ulink></listitem>
</itemizedlist>

</section> <!-- soft_queue_maui -->

</section> <!-- soft_queue -->

<section id="soft_libs"><title>Библиотеки</title>
<section id="soft_libs_openmpi"><title>OpenMPI</title>
<command>OpenMPI</command> (Open Multi-Processing) — открытый стандарт для распараллеливания программ на языках Си, Си++ и Фортран.
Описывает совокупность директив компилятора, библиотечных процедур и переменных окружения, которые предназначены 
для программирования многопоточных приложений на многопроцессорных системах с общей памятью.
<itemizedlist>
 <listitem> Версия: 1.4.4</listitem>
 <listitem> Расположение: <filename>/usr/lib64/openmpi/1.4-gcc</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.open-mpi.org/">WWW</ulink>, <link linkend="ex_compile-pi">Пример компиляции</link>, <link linkend="ex_pi">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_libs_openmpi -->

<section id="soft_libs_mpich"><title>MPICH2</title>
<command>MPICH2</command> (Message Passing Interface Chameleon) — это одна из самых первых разработанных библиотек MPI. 
На её базе было создано большое количество других библиотек как OpenSource, так и коммерческих.
В настоящее время существует две ветви исходных кодов: MPICH1 и MPICH2. Разработка ветви MPICH1 заморожена. 
Ветвь MPICH2 активно разрабатывается в Арагонской лаборатории, с участием IBM, Cray, SiCortex, Microsoft, Intel, 
NetEffect, Qlogic, Myricom, Ohio state university, UBC.
<itemizedlist>
 <listitem> Версия: 1.2.1p1, 1.4</listitem>
 <listitem> Расположение: <filename>/usr/lib/mpich2</filename>, <filename>/usr/local/mpich2-1.4.20110620</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.mcs.anl.gov/research/projects/mpich2/">WWW</ulink></listitem>
</itemizedlist>
</section> <!-- soft_libs_mpich -->
</section> <!-- soft_libs -->

<section id="soft_compilers"><title>Компиляторы</title>
<section id="soft_compilers_intel"><title>Intel Composer XE</title>
Средство <command>Intel® Composer XE 2011</command> сочетает в себе оптимизированный компилятор, высокопроизводительные библиотеки,
расширенные средства векторизации, поддержку OpenMP* и Intel® Parallel Building Blocks. Оно позволяет быстро и
легко оптимизировать производительность приложения для многоядерных процессоров для разных ОС с одной базой кода.
В пакет входят компиляторы C, C++, Fortran и библиотека MKL.
<itemizedlist>
 <listitem> Версия: 12.0.1.107, 12.0.2.137</listitem>
 <listitem> Расположение: <filename>/usr/local/intel/</filename></listitem>
 <listitem> Ссылки: <ulink url="http://software.intel.com/en-us/articles/intel-composer-xe/">WWW</ulink>, <link linkend="ex_intel">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- Intel Composer XE -->

<section id="soft_compilers_gcc"><title>GNU Compiler Collection</title>
<command>GNU Compiler Collection</command> (обычно используется сокращение <command>GCC</command>) — набор компиляторов для различных языков 
программирования, разработанный в рамках проекта GNU. <command>GCC</command> является свободным программным обеспечением, 
распространяется фондом свободного программного обеспечения (FSF) на условиях GNU GPL и GNU LGPL и является 
ключевым компонентом GNU toolchain. Он используется как стандартный компилятор для свободных UNIX-подобных 
операционных систем.
В пакет входят компиляторы C, C++, Fortran и др.
<itemizedlist>
 <listitem> Версия: 4.1, 4.4</listitem>
 <listitem> Расположение: <filename>/usr/</filename></listitem>
 <listitem> Ссылки: <ulink url="http://gcc.gnu.org/">WWW</ulink></listitem>
</itemizedlist>
</section> <!-- soft_compilers_gcc -->

<section id="soft_compilers_nag"><title>NAG Fortran Compiler</title>
<command>NAG Fortran Compiler</command> надежный и эффективный компилятор, признанный разработчиками 
во всем мире. Он поддерживает 77, 90, 95 и практически полностью 2003 стандарты языка Fortran. 
Написан на языке C. Доступен для различных UNIX платформ. NAG Fortran Compiler обеспечивает 
высокую скорость компиляции и эффективность скомпилированных программ. Также данный компилятор 
поддерживает High Performance Fortran и ряд других расширений.
<itemizedlist>
 <listitem> Версия: 5.2</listitem>
 <listitem> Расположение: <filename>/usr/local/NAG/</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.nag.co.uk/nagware/np.asp">WWW</ulink>, <link linkend="ex_nag">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_compilers_nag -->
<section id="soft_compilers_pgi"><title>PGI</title>
Пакет <command>PGI</command> — набор из компиляторов для языков C, C++, Fortran, HPF, параллельного отладчика pgdbg, 
профилера pgprof, предназначенных для создания и отладки программ в параллельной распределенной среде.
<itemizedlist>
 <listitem> Версия: 11.10</listitem>
 <listitem> Расположение: <filename>/usr/local/pgi</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.pgroup.com/">WWW</ulink>, <link linkend="ex_pgi">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_compilers_pgi -->

<section id="soft_compilers_intel_ics">
<title>Intel Cluster Studio</title>
Intel Cluster Studio включает в себя набор интсрументов для разработки параллельных приложений, работающих на кластере или многопроцессорной среде.
Пакет предоставляет следующие программы (многие из них имеют графический интерфейс):
<itemizedlist>
	<listitem>icc, icpc, ifort — компиляторы для языков C, C++, Fortran</listitem>
	<listitem>idb — отладчик</listitem>
	<listitem>amplxe-gui — профайлер и анализатор многопоточных программ</listitem>
	<listitem>traceanalyzer — профайлер программ MPI</listitem>
	<listitem>mpitune — оптимизатор программ MPI под конкретный кластер</listitem>
</itemizedlist>
Также в пакете доступны следующие библиотеки:
<itemizedlist>
	<listitem>Math Kernel Library (MKL), включающая в себя ScaLAPACK, BLAS, FFTW и др.</listitem>
	<listitem>библиотека MPI для кластерных приложений</listitem>
	<listitem>Thread Building Blocks (TBB) для многопоточных приложений</listitem>
	<listitem>Integrated Performance Primitives (IPP) для ускорения обработки мультимедийных данных</listitem>
</itemizedlist>
<itemizedlist>
 <listitem> Версия: 12.1.0 </listitem>
 <listitem> Расположение: <filename>/usr/local/</filename></listitem>
 <listitem> Ссылки: <link linkend="ex_compilers_intel_ics">Пример запуска</link></listitem>
 <listitem> Документация:
    <itemizedlist>
	<listitem><ulink url="http://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/">Сборка приложений с библиотекой MKL</ulink></listitem>
	<listitem><ulink url="http://software.intel.com/en-us/articles/intel-math-kernel-library-documentation/">MKL</ulink></listitem>
	<listitem><ulink url="http://software.intel.com/en-us/articles/intel-threading-building-blocks-documentation/">TBB</ulink></listitem>
	<listitem><ulink url="http://software.intel.com/en-us/articles/intel-integrated-performance-primitives-documentation/">IPP</ulink></listitem>
	<listitem><ulink url="http://software.intel.com/en-us/articles/intel-mpi-library-documentation/">MPI</ulink></listitem>
	<listitem><ulink url="http://software.intel.com/en-us/articles/intel-c-compiler-professional-edition-for-linux-documentation/">компилятор C, C++</ulink></listitem>
	<listitem><ulink url="http://software.intel.com/en-us/articles/intel-fortran-compiler-professional-edition-for-linux-documentation/">компилятор Fortran</ulink></listitem>
	<listitem><ulink url="http://software.intel.com/sites/products/documentation/hpc/clusterxe/ICSXE2012Linux_GettingStarted.pdf">краткая документация по всем инструментам ICS</ulink></listitem>
    </itemizedlist>
 </listitem>
</itemizedlist>
</section> <!-- soft_compilers_intel_ics -->
</section> <!-- soft_compilers -->

<section id="soft_apps"><title>Пакеты прикладных программ</title>
<section id="soft_apps_abinit"><title>Abinit</title>
<command>Abinit</command> — свободное ПО, распространяемое по GNU General Public License3 и предназначенное для расчётов полной 
энергии, электронной плотности и т. д. систем электронов и ядер (с использованием периодических граничных 
условий) в рамках метода функционала плотности с использованием базиса из плоских волн и псевдопотенциалов.
Abinit позволяет оптимизировать геометрию системы минимизируя силы или напряжения, проводить 
молекулярно-динамическое моделирование, вычислять распределение электронной плотности, определять 
динамическую матрицу, эффективный заряд и многое другое.
<itemizedlist>
 <listitem> Версия: 6.8.1</listitem>
 <listitem> Расположение: <filename>/usr/local/abinit-6.8.1</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.abinit.org/">WWW</ulink>, <link linkend="ex_abinit">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_abinit -->

<section id="soft_apps_ff"><title>Firefly</title>
программный пакет для ab initio квантовохимических расчётов. Работает на Intel-совместимых процессорах 
архитектур x86 и x86-64 (только 32-битная версия). Основана на коде пакета программ GAMESS (US). Автором 
было переписано 60-70% кода. Наибольшие изменения коснулись платформозависимых частей программы 
(выделение памяти, дисковый ввод-вывод, параллельный запуск), математических функций (например, 
матричные операции) и квантовохимических методов (метод Хартри-Фока, теория Мёллера-Плессе 
(англ. Møller–Plesset perturbation theory), теория функционала плотности). Вследствие этого PC GAMESS 
стал значительно быстрее, чем оригинальная программа. Основной разработчик программы — Александр Грановский.
С октября 2008 года проект дистанцировался от GAMESS (US) и поменял имя на Firefly. До 17 октября 2009 оба
имени использовались наравне друг с другом.
<itemizedlist>
 <listitem> Версия: 7.1.G</listitem>
 <listitem> Расположение: <filename>/usr/local/firefly/mpich2</filename></listitem>
 <listitem> Ссылки: <ulink url="http://classic.chem.msu.su/gran/gamess/">WWW</ulink>, <link linkend="ex_ff">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_ff -->

<section id="soft_apps_g03"><title>Gaussian</title>
<command>Gaussian</command> (читается как гауссиан) &#160;&#8212; компьютерная программа для расчета структуры и свойств молекулярных 
систем, включающая большое разнообразие методов вычислительной химии, квантовой химии, молекулярного 
моделирования. Создана нобелевским лауреатом Джоном Поплом и его исследовательской группой и с тех пор 
постоянно обновляется. <command>Gaussian 09</command> &#160;&#8212; самая последняя реализация программы.
<itemizedlist>
 <listitem> Версия: 03</listitem>
 <listitem> Расположение: <filename>/usr/local/g03/</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.gaussian.com/">WWW</ulink>, <link linkend="ex_g03">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_g03 -->

<section id="soft_apps_scilab"><title>Scilab</title>
<command>Scilab</command> &#160;&#8212; пакет прикладных математических программ, предоставляющий мощное открытое окружение для
инженерных (технических) и научных расчётов.
<itemizedlist>
 <listitem> Версия: 5.3.0</listitem>
 <listitem> Расположение: <filename>/usr/local/scilab-5.3.0/</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.scilab.org/">WWW</ulink>, <link linkend="ex_array-scilab">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_scilab -->

<section id="soft_apps_wien2k"><title>WIEN2k</title>
<command>WIEN2k</command> – программный пакет для расчетов электронных структур в твердом веществе, использующий теорию
функционала плотности (DFT). Основан на методе (линеаризованных) присоединенных плоских волн полного
потенциала ((L)APW) + локальных орбиталей (lo), что является одной из наиболее точных схем для расчетов
зонных структур. В DFT может быть использована локальная аппроксимация (спиновой) плотности (LDA) или
улучшенная версия генерализованной аппроксимации градиента (GGA). Пакет <command>WIEN2k</command> использует полностью
электронную схему, включая релятивистские эффекты и имеет много достоинств. Грид-порт пакета включает
прототип последовательности операций для работы в гриде. Пакет разрешено использовать только обладателям
действительной лицензии <command>WIEN2k</command>.
<itemizedlist>
 <listitem> Версия: 11</listitem>
 <listitem> Расположение: <filename>/usr/local/WIEN2k/WIEN2k_11/</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.wien2k.at/">WWW</ulink>, <link linkend="ex_wien2k">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_wien2k -->

<section id="soft_apps_ANSYS"><title>ANSYS</title>
<command>ANSYS</command> — универсальная программная система конечно-элементного (МКЭ) анализа, существующая и развивающаяся на протяжении последних 30 лет, является довольно популярной у специалистов в области компьютерного инжиниринга (CAE, Computer-Aided Engineering) и КЭ решения линейных и нелинейных, стационарных и нестационарных пространственных задач механики деформируемого твёрдого тела и механики конструкций (включая нестационарные геометрически и физически нелинейные задачи контактного взаимодействия элементов конструкций), задач механики жидкости и газа, теплопередачи и теплообмена, электродинамики, акустики, а также механики связанных полей. Моделирование и анализв некоторых областях промышленности позволяет избежать дорогостоящих и длительных циклов разработки типа «проектирование — изготовление — испытания». Система работает на основе геометрического ядра Parasolid.
<itemizedlist>
 <listitem> Версия: 130</listitem>
 <listitem> Расположение: <filename>/usr/local/ansys_inc/v130/ansys</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.ansys.com/">WWW</ulink></listitem>
</itemizedlist>
</section> <!-- soft_apps_ANSYS -->

<section id="soft_apps_Crystal"><title>Crystal</title>
<command>Crystal</command> – это научный программный пакет для расчетов в области квантовой химии твердого тела. Разработан специально для моделирования 3- и 2-периодических кристаллических решёток и 1-периодических полимеров. Crystal позволяет вычислять энергию основного состояния, ее изменение, а также волновые функции и параметры периодических систем. Написан и поддерживается группой итальянских учёных с 70-х годов XX века.
<itemizedlist>
 <listitem> Версия: 09</listitem>
 <listitem> Расположение: <filename>/usr/local/crystal09</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.crystal.unito.it/">WWW</ulink>, <ulink url="http://www.cryscor.unito.it/cms/index.php?id=49">Тестовые задачи</ulink>, <ulink url="http://www.theochem.unito.it/crystal_tuto/mssc2008_cd/tutorials/index.html">Учебные материалы</ulink>, <link linkend="ex_crystal">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_ANSYS -->

<section id="soft_apps_molpro"><title>Molpro</title>
<command>Molpro</command> - программа для ab initio квантово-химических расчетов. 
Основные разработчики: H.-J. Werner и P. J. Knowles (университет Штутгарта). 
Основное отличие от аналогичных программ - 
особое внимание к корреляционным эффектам (мультиконфигурационное взаимодействие,
метод объединенных кластеров ... ). Эффективна для расчета больших молкулярных систем. 
Доступны как последовательная (molpros_2010_1_Linux_x86_64_i8), 
так и параллельная (molprop_2010_1_Linux_x86_64_i8) версии.
<itemizedlist>
 <listitem> Версия: 2010.1</listitem>
 <listitem> Расположение: <filename>/usr/local/molpro</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.molpro.net/">WWW</ulink>, <link linkend="ex_molpro">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_molpro -->

<section id="soft_apps_openfoam"><title>OpenFOAM</title>
<command>OpenFOAM</command> (Open Field Operation and Manipulation) - открытая интегрируемая платформа для численного 
моделирования задач механики сплошных сред. OpenFOAM — свободно распространяемый инструментарий вычислительной гидродинамики 
для операций с полями. Он позволяет решать следующие задачи: прочностные расчеты, задачи гидродинамики и аэродинамики, 
теплопроводности и т.д. В основе кода лежит набор библиотек, предоставляющих инструменты для решения систем дифференциальных 
уравнений в частных производных как в пространстве, так и во времени. Написан на C++.
<itemizedlist>
 <listitem> Версия: 2.1.0</listitem>
 <listitem> Расположение: <filename>/usr/local/OpenFOAM</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.openfoam.org">WWW</ulink>, <link linkend="ex_openfoam">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_openfoam -->

<section id="soft_apps_concorde"><title>Concorde</title>
<command>Concorde</command> - пакет для решения задачи коммивояжера и некоторых других 
подобных оптимизационных задач. Код написан на ANSI C и доступен для академических целей.
Библиотека Concorde включает более 700 функций, которые могут быть использованы
для создания программ, нацеленных на решение подобных задач.	
<itemizedlist>
 <listitem> Версия: 031219</listitem>
 <listitem> Расположение: <filename>/usr/local/Concorde/concorde</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.tsp.gatech.edu/concorde/index.html">WWW</ulink>, <link linkend="ex_concorde">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_concorde -->

<section id="soft_apps_gromacs"><title>GROMACS</title>
<command>GROMACS</command> (GROningen MAchine for Chemical Simulations, дословный перевод названия &#160;&#8212; Гронингенская 
Машина для Химического Моделирования) пакет молекулярной динамики для моделирования физико-химических 
процессов, первоначально разработанный группой Германа Берендсена из департамента биофизической химии 
Гронингенского университета, сейчас развивается и поддерживается благодаря усилиям энтузиастов из разных 
стран, включая университет Уппсалы, Королевский технологический институт. Пакет предназначался в основом 
для моделирования биомолекул (белки и липиды), имеющих много связанных взаимодействий между атомами, но, 
так как <command>GROMACS</command> обеспечивает высокую скорость расчетов для несвязанных взаимодействий, считается, что 
это один из самых быстрых инструментов. <command>GROMACS</command> является программным обеспечением с открытым исходным 
кодом, выпущенным под лицензией GPL.
<itemizedlist>
 <listitem> Версия: 4.5.5</listitem>
 <listitem> Расположение: <filename>/usr/local/GROMACS/</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.gromacs.org/">WWW</ulink>, <link linkend="ex_gromacs">Пример запуска</link></listitem>
</itemizedlist>
</section> <!-- soft_apps_gromacs -->
</section> <!-- soft_apps -->

<section id="soft_planed"><title>ПО планируемое к установке</title>
<section id="soft_apps_mdynamix"><title>MDynaMix</title>
<command>MDynaMix</command> (Molecular Dynamics of Mixtures)
<itemizedlist>
 <listitem> Версия: 5.2 (18.12.2009)</listitem>
 <listitem> Расположение: <filename>/usr/local/md52</filename></listitem>
 <listitem> Ссылки: <ulink url="http://www.fos.su.se/~sasha/mdynamix/">WWW</ulink></listitem>
</itemizedlist>
</section> <!-- soft_apps_mdynamix -->
</section> <!-- soft_planed -->
</section> <!-- soft -->

<section id="hard"><title>Аппаратное обеспечение</title>

<section id="hard_vm"><title>Кластер виртуальных машин</title>
Вычислительный модуль HP (Шкафы HP 10647 G2, 47U).
<itemizedlist>
 <listitem> Две корзины блейд-серверов состоят из:
    <itemizedlist>
      <listitem> HP BladeSystem c7000</listitem>
      <listitem> VC-Flex-10 (10 GbE)</listitem>
      <listitem> QDR IB</listitem>
      <listitem> 16xBL460G7 (2 x X5670, 96GB RAM, 2 x 10GbE, 2 x QDR IB)</listitem>
    </itemizedlist>
 </listitem>
  <listitem> Две системы хранения HP состоят из:
    <itemizedlist>
      <listitem> StorageWorks P4500 G2</listitem>
      <listitem> 240 TB сырой ёмкости (120 x 2TB SAS HDD)</listitem>
    </itemizedlist>
  </listitem>
  <listitem> Два коммутатора ProCurve E6600-48G-4XG (48 x 1GbE и 4 x 10GbE)</listitem>
  <listitem> Два  источника бесперебойного питания R12000/3 (3-фазное питание 12КВт)</listitem>
</itemizedlist>

Итого: 32 сервера, 64 процессора, 384 ядра, 3 TB RAM, 480 TB HDD, 40Gbps IB, пиковая производительность 4,5TFlop
</section> <!-- hard_vm -->

<section id="hard_tp"><title>Кластер Т-Платформ</title>
Вычислительный кластер Т-Платформы T-EDGE96 HPC-0011828-001.
<itemizedlist>
 <listitem>  48 вычислительных узлов. Всего: 96 процессоров, ядер 384, 768 Гб ОЗУ, 7,68 Тб дискового пространства.
  <itemizedlist>
    <listitem> Размер: 2U</listitem>
    <listitem> Процессор: 2 процессора E5335 2.0ГГц</listitem>
    <listitem> ОЗУ: 16 Гб</listitem>
    <listitem> Диск: 160 Гб</listitem>
  </itemizedlist>
  </listitem>
  <listitem> Управляющий узел 2 x X5640, 8 ГБ RAM, 3 ТБ</listitem>
  <listitem> Коммутаторы Gigabit Ethernet DGS-3324SR (10/100/1000), DGS-1224T</listitem>
  <listitem> Коммутаторы Infiniband Mellanox MHGS18-XSC 20 Гбит/с, полный дуплекс 10 Гбит/с, 4X SDR, F-X430046, F-X430047</listitem>
  <listitem> Аппаратные шкафы (АШ) APC, NetShelter SX 19"42U, AR3100</listitem>
</itemizedlist>

Итого: 48 серверов, 96 проц, 384 ядра, 768 ГБ RAM, 7,68 ТБ HDD, 20Gbps IB, пиковая производительность 3,07TFlops
</section> <!-- hard_tp -->

<section id="hard_itanium"><title>Кластер Itanium</title>
Один вычислительный узел IBM xSeries 450.
<itemizedlist>
  <listitem> Размер: 4U</listitem>
  <listitem> Процессор: 4 ядра Itanium 2 1.4 ГГц</listitem>
  <listitem> ОЗУ: 8 Гб</listitem>
  <listitem> Диск: 40 Гб</listitem>
</itemizedlist>
</section> <!-- hard_itanium -->

</section> <!-- hard -->

<!-- /section --> <!-- Описание сервиса Высокопроизводительные вычисления -->


</article>
